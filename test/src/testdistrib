#!/usr/bin/env python
#
# (c) Copyright 2008 Cloudera, Inc
# testdistrib  -- runs test processes across the Cloudera Hadoop Distribution

import logging
import os
import sys
import tempfile

import com.cloudera.tools.ec2 as ec2
import com.cloudera.util.output as output

import com.cloudera.distribution.sshall as sshall
import com.cloudera.distribution.scpall as scpall

from   distrotester.constants import *
import distrotester.platform as platform
from   distrotester.testerror import TestError
import distrotester.testproperties as testproperties

# do this first
output.initLogging()

# use all args except program name
argv = sys.argv[1:]

properties = testproperties.TestProperties()
output.attachOutputArgParser(properties)

testproperties.loadAllProperties(properties, argv)


# if the user has not selected a log file name and a log level,
# force a log file here. Also set the default log level to verbose.
setLogName = properties.getProperty(output.LOG_FILENAME_PROP)
if setLogName == None:
  properties.setProperty(output.LOG_FILENAME_PROP, DEFAULT_LOG_FILENAME)

setLogVerbosity = properties.getProperty(output.LOG_VERBOSITY_PROP)
if setLogVerbosity == None:
  properties.setProperty(output.LOG_VERBOSITY_PROP, DEFAULT_LOG_VERBOSITY)

# set up the console. Also starts logging to the specified file
output.setupConsole(properties)

# if we are here, then properties and argv parsing succeeded

localDistribTarball = properties.getProperty(DISTRIB_TARBALL_KEY)
if localDistribTarball != None:
  localDistribTarball = os.path.abspath(localDistribTarball)


# Where are we? Switch into test program's base dir
binName = sys.argv[0]
binDir = properties.getProperty(TEST_BINDIR_KEY, os.path.dirname(binName))
os.chdir(binDir)

# Did the user ask for --list-platforms? If so, list 'em and exit.
if properties.getBoolean(LIST_PLATFORMS_KEY):
  platform.listPlatforms()
  sys.exit(1)

# Make sure we have AWS credentials; we'll need these after we make an instance
awsSecretKey = properties.getProperty(AWS_SECRET_ACCESS_KEY)
awsAccessKey = properties.getProperty(AWS_ACCESS_KEY_ID)
awsAccountId = properties.getProperty(AWS_ACCOUNT_ID_KEY)
packageBucket = properties.getProperty(PACKAGE_BUCKET_KEY, \
    PACKAGE_BUCKET_DEFAULT)

if awsSecretKey == None:
  logging.error("Error: " + AWS_SECRET_KEY_ENV + " is not set")
  sys.exit(1)
if awsAccessKey == None:
  logging.error("Error: " + AWS_ACCESS_KEY_ENV + " is not set")
  sys.exit(1)
if awsAccountId == None:
  logging.error("Error: " + AWS_ACCOUNT_ID_ENV + " is not set")
  sys.exit(1)

if localDistribTarball == None:
  logging.error("Error: distribution location was not set with " \
      + DISTRIB_TARBALL_ARG)
  sys.exit(1)

# ok, time to actually make the work.

# Make a tarball of the test harness.
(oshandle, tarballFilename) = tempfile.mkstemp(".tar.gz", "harness-")
try:
  handle = os.fdopen(oshandle, "w")
  handle.close()
except OSError:
  # irrelevant
  pass
except IOError:
  # irrelevant
  pass

harnessBaseDir = os.path.abspath(os.path.join(os.getcwd(), HARNESS_BASE_DIR))
harnessParentDir = os.path.abspath(os.path.join(harnessBaseDir, "..")
baseName = os.path.basename(harnessBaseDir)

# Actually tar it up.
cmd = "tar czf \"" + tarballFilename + "\" -C \"" + harnessParentDir + "\""\
    + " \"" + baseName + "\""
shell.sh(cmd)

# start an EC2 instance for this process.
platformName = properties.getProperty(TEST_PLATFORM_KEY)
if platformName == None:
  logging.error("Error: No platform specified with " + TEST_PLATFORM_ARG)
  sys.exit(1)

logging.info("Launching tests on platform: " + platformName)

existingInstanceNames = properties.getProperty(EXISTING_INSTANCES_KEY)
if existingInstanceNames != None:
  instances = existingInstanceNames.split(",")
  userSuppliedInstances = True
  logging.info("Using instances:")
else:
  instances = platform.launchInstances(platformName, properties)
  userSuppliedInstances = False
  logging.info("Started instances:")

# TODO: Documentation: We need to set EC2_HOME, EC2_PRIVATE_KEY, EC2_CERT
# for any user who will be running this tool (Including autotest users)
for instance in instances:
  logging.info("  " + instance)
instanceLines = listInstances(None, properties)

dnsMap = ec2.getInstanceDnsNames(instanceLines, instances)

# We have a map from instanceId -> (internal dns, external dns)
# Condense this into a list we can use.
# Determine whether we should use EC2 internal dns names or external names
# based on whether we're inside EC2 or not. (which we simply check for a
# 10.x.y.z address for. TODO (aaron): This is not a foolproof method)
useInternal = False
try:
  myHostNameLines = shell.sh("hostname -i")
  if len(myHostNameLines) > 0:
    hostLine = myHostNameLines[0].strip()
  if hostLine.startswith("10."):
    useInternal = True
except shell.CommandError:
  pass

hostList = []
for (internal, external) in dnsMap.values():
  if useInternal:
    hostList.append(internal)
  else:
    hostList.append(external)
if len(hostList) == 0:
  logging.error("Empty host list? Don't know how to continue!")
  sys.exit(1)

# Everything from here out must be guarded to terminate the nodes
# if the user didn't create them.
try:

  # Upload test harness tarball.
  try:
    try:
      scpall.scpMultiHosts(tarballFilename, "root", hostList, \
        remoteHarnessTarFile, properties, SCP_RETRIES, SCP_PARALLEL)
    except scpall.MultiScpError, mse:
      failedHosts = mse.getFailedHostList()
      logging.error("Could not upload test harness to the following hosts:")
      for host in failedHosts:
        logging.error("  " + host)
      raise
  finally:
    os.remove(tarballFilename)

  # Run platform-specific bootstrap commands. Append the unzip commands here
  # so that at the end of this, we know that we're ready to go on all nodes.
  platformSetup = platform.setupForPlatform(platformName)
  bootstrapCmds = platformSetup.remoteBootstrap()
  bootstrapCmds.append(["tar -zxf \"/mnt/" + tarballFilename + \" -C /mnt")

  for cmd in bootstrapCmds:
    results = sshall.sshMultiHosts("root", hostList, cmd, properties, \
        SSH_RETRIES, SSH_PARALLEL)
    for result in results:
      # each result is an sshall.SshResult object
      if result.getStatus() != 0:
        logging.error("Got error result executing bootstrap cmd on: " \
            + result.getHost())
        logging.error("Command was: " + result.getCommand())

  # Run platform-specific installation functions on all nodes.
  cmd = REMOTE_SETUP_COMMAND + " " + TEST_PLATFORM_ARG + " " + platformName \
      + " " + AWS_SECRET_KEY_ARG + " '" + awsSecretKey + "'" \
      + " " + AWS_ACCESS_KEY_ARG + " '" + awsAccessKey + "'" \
      + " " + AWS_ACCOUNT_ID_ARG + " '" + awsAccountId + "'" \
      + " " + PACKAGE_BUCKET_ARG + " " + packageBucket

  results = sshall.sshMultiHosts("root", hostList, cmd, properties, \
      SSH_RETRIES, SSH_PARALLEL)
  for result in results:
    # each result is an sshall.SshResult object
    if result.getStatus() != 0:
      logging.error("Got error result running platform install on: " \
          + result.getHost())
      logging.error("Command was: " + result.getCommand())

  # Pick one node to the "master" node.
  masterHost = hostList[0]
  numHosts = len(hostList)
  slaveAddrs = hostList[1:numHosts]

  # write slaves list to a file and upload to that node.
  (oshandle, tmpFilename) = tempfile.mkstemp("", "slaves-")
  handle = os.fdopen(oshandle, "w")
  for addr in slaveAddrs:
    handle.write(addr + "\n")
  handle.close()
  remoteSlavesName = os.path.join("/mnt/", os.path.basename(tmpFilename))

  shell.scp(tmpFilename, "root", masterHost, remoteSlavesName, properties)

  # we no longer need the list of slave addrs locally.
  os.remove(tmpFilename)

  # Upload distro tarball to master host.
  remoteDistribTarball = os.path.join("/mnt/", \
      os.path.basename(localDistribTarball)
  shell.scp(localDistribTarball, "root", masterHost, remoteDistribTarball, \
      properties)

  # Run the global test battery loop on the master node
  cmd = REMOTE_TEST_COMMAND + " " + TEST_PLATFORM_ARG + " " + platformName \
      + " " + AWS_SECRET_KEY_ARG + " '" + awsSecretKey + "'" \
      + " " + AWS_ACCESS_KEY_ARG + " '" + awsAccessKey + "'" \
      + " " + AWS_ACCOUNT_ID_ARG + " '" + awsAccountId + "'" \
      + " " + PACKAGE_BUCKET_ARG + " " + packageBucket \
      + " " + SLAVES_FILE_ARG + " " + remoteSlavesName \
      + " " + DISTRIB_TARBALL_ARG + " " + remoteDistribTarball
  shell.ssh("root", masterHost, cmd, properties)

finally:
  # shut down any instances we started. We're responsible for them.
  # TODO: If this was run by a user, only shut down on success.
  # If this is run in unattended mode, always shut down.
  if not userSuppliedInstances:
    logging.info("Shutting down instances...")
    ec2.terminateInstances(instances, properties)


sys.exit(0)

