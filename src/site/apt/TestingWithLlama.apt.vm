~~ Licensed under the Apache License, Version 2.0 (the "License");
~~ you may not use this file except in compliance with the License.
~~ You may obtain a copy of the License at
~~
~~ http://www.apache.org/licenses/LICENSE-2.0
~~
~~ Unless required by applicable law or agreed to in writing, software
~~ distributed under the License is distributed on an "AS IS" BASIS,
~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
~~ See the License for the specific language governing permissions and
~~ limitations under the License.

  ---
  Llama, Testing with
  ---
  ---
  ${maven.build.timestamp}

Testing with Llama

%{toc|section=1|fromDepth=2}

* Using MiniLlama from within Java Testcases
  
  Add the following dependency in your project
  
+----+
    <dependency>
      <groupId>com.cloudera.llama</groupId>
      <artifactId>llama-thrift-mini-am</artifactId>
      <version>${project.version}</version>
      <scope>test</scope>
    </dependency>
+----+

  The Maven repository where SNAPSHOT artifacts are currently deployed is 
  <<<{{https://repository.cloudera.com/artifactory/libs-snapshot-local}}>>>

  <<IMPORTANT:>> By default, MiniLlama uses an ephemeral port bound to
  <<<localhost>>> only.

**  Testcase boiler plate code:
  
+----+
public class TestMiniLlamaWithMock {

  private Configuration createMiniLlamaConfiguration() {
     ...
  }
  
  @Test
  public void testMiniLlama() throws Exception {
    Configuration conf = createMiniLlamaConfiguration();
    try {
      server.start();
      
      String serverHost = server.getAddressHost();
      String serverPort = server.getAddressPort();
      
      TTransport transport = new TSocket(server.getAddressHost(),
          server.getAddressPort());
      transport.open();
      TProtocol protocol = new TBinaryProtocol(transport);
      LlamaAMService.Client client = new LlamaAMService.Client(protocol);
      ....
    } finally {
      server.stop();
    }   
  }
}
+----+

** Using MiniLlama with MockLlamaAM
  
  You need to create a Mock configuration.
  
  For example:
  
+----+
public class TestMiniLlamaWithMock {

  private Configuration createMiniLlamaConfiguration() {
    Configuration conf = MiniLlama.createMockConf(
        Arrays.asList("queue1", "queue2"), Arrays.asList("node1", "node2"));
  }

  @Test
  public void testMiniLlama() throws Exception {
    ...
  }
}
+----+

** Using MiniLlama with YarnLlamaAM
  
  Using MiniLlama with the YarnLlamAM will also start a Hadoop HDFS/Yarn 
  minicluster. You need to specify how many nodes (number of DataNodes and 
  NodeManagers) for the Hadoop HDFS/Yarn minicluster.
  
  <<NOTE:>> Until {{{https://issues.apache.org/jira/browse/YARN-1008}YARN-1008}}
  is integrated, only one node miniclusters are supported.
  
  You must have in your classpath (in a directory, not in a JAR) a 
  <<<fair-scheduler-allocation.xml>>> file defining the FairScheduler queue
  configuration.
  
  For example:
  
+----+
public class TestMiniLlamaWithMock {

  private Configuration createMiniLlamaConfiguration() {
    URL url = Thread.currentThread().getContextClassLoader().getResource(
        "fair-scheduler-allocation.xml");
    String fsallocationFile = url.toExternalForm();
    fsallocationFile = fsallocationFile.substring("file://".length());
    Configuration conf = MiniLlama.createMiniClusterConf(1);
    conf.set("yarn.scheduler.fair.allocation.file", fsallocationFile);
    conf.set(LlamaAM.INITIAL_QUEUES_KEY, "default");
    return conf;
  }
  
  @Test
  public void testMiniLlama() throws Exception {
    ...
  }
}
+----+
  
  Example of a minimal <<<fair-scheduler-allocation.xml>>> configuration file 
  defining 3 queues:
  
+----+
<allocations>
  <queue name="queue1">
  </queue>
  <queue name="queue2">
  </queue>
  <queue name="queue3">
  </queue>
</allocations>
+----+

* Using MiniLlama from the Command Line

  <<IMPORTANT:>> MiniLlama from the Command Line is only available from the
  standalone TARBALL, it is not available in the packaging TARBALL.

** Install MiniLlama

  Expand the <<<llama-${project.version}-standalone.tar.gz>>> TARBALL.

** Configure MiniLlama

  MiniLlama uses the following 2 configuration files from Llama installation
  directory:

  * <<<conf/llama-site.xml>>>

  * <<<conf/minillama-log4j.properties>>>

** Run MiniLlama

  To run MiniLlama use the <<<minillama>>> script:

+---+
$ bin/minillama --hadoop-conf=/Users/tucu/myhadoopconf --hadoop-nodes=3
+---+

  The 2 options shown are required.

  The <<<--hadoop-conf>>> option must point to a Hadoop configuration defining
  the configuration for the HDFS minicluster and Yarn minicluster.

  <<IMPORTANT:>> Hadoop core or hdfs configuration files must define the
  <<<fs.defaultFS>>> property.

  <<IMPORTANT:>> The yarn configuration must define the scheduler configuration
  and it should not include Map-Reduce's <<<ShuffleHandler>>> auxiliary service
  to avoid port conflicts.

  The <<<--hadoop-nodes>>> option indicates how many DataNodes and NodeManagers
  the minicluster should have.

  Once HDFS, Yarn and Llama are running the logs will output the following
  message:

+---+
2013-09-02 09:14:47,227 INFO  [main] server.MiniLlama (MiniLlama.java:main(62)) - *********************************************************************************************************************
2013-09-02 09:14:47,244 INFO  [main] server.MiniLlama (MiniLlama.java:main(64)) - Mini Llama running with HDFS/Yarn minicluster with 2 nodes, HDFS URI: hdfs://localhost:8020 Llama URI: 0.0.0.0:15000
2013-09-02 09:14:47,244 INFO  [main] server.MiniLlama (MiniLlama.java:main(68)) - *********************************************************************************************************************
+---+
