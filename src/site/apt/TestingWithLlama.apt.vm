~~ Licensed under the Apache License, Version 2.0 (the "License");
~~ you may not use this file except in compliance with the License.
~~ You may obtain a copy of the License at
~~
~~ http://www.apache.org/licenses/LICENSE-2.0
~~
~~ Unless required by applicable law or agreed to in writing, software
~~ distributed under the License is distributed on an "AS IS" BASIS,
~~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
~~ See the License for the specific language governing permissions and
~~ limitations under the License.

  ---
  Llama, Testing with
  ---
  ---
  ${maven.build.timestamp}

Testing with Llama

%{toc|section=1|fromDepth=2}

* Using MiniLlama from within Java Testcases
  
  Add the following dependency in your project
  
+----+
    <dependency>
      <groupId>com.cloudera.llama</groupId>
      <artifactId>mini-llama</artifactId>
      <version>${project.version}</version>
      <scope>test</scope>
    </dependency>
+----+

  The Maven repository where SNAPSHOT artifacts are currently deployed is 
  <<<{{https://repository.cloudera.com/artifactory/libs-snapshot-local}}>>>

  <<IMPORTANT:>> By default, MiniLlama uses an ephemeral port bound to
  <<<localhost>>> only.

**  Testcase boiler plate code:
  
  Using MiniLlama will also start a Hadoop HDFS/Yarn minicluster. You need to 
  specify how many nodes (number of DataNodes and NodeManagers) for the Hadoop 
  HDFS/Yarn minicluster.
  
  You must have in your classpath (in a directory, not in a JAR) a 
  <<<fair-scheduler.xml>>> file defining the FairScheduler queue
  configuration.
  
  For example:
  
+----+
public class TestUsingMiniLlama {

  private Configuration createMiniLlamaConfiguration() {
    URL url = Thread.currentThread().getContextClassLoader().getResource(
        "fair-scheduler.xml");
    String fsallocationFile = url.toExternalForm();
    fsallocationFile = fsallocationFile.substring("file://".length());
    Configuration conf = MiniLlama.createMiniClusterConf(1);
    conf.set("yarn.scheduler.fair.allocation.file", fsallocationFile);
    conf.set(LlamaAM.INITIAL_QUEUES_KEY, "default");
    return conf;
  }
  
  @Test
  public void testMiniLlama() throws Exception {
    Configuration conf = createMiniLlamaConfiguration();
    try {
      server.start();
      
      String serverHost = server.getAddressHost();
      String serverPort = server.getAddressPort();
      
      TTransport transport = new TSocket(server.getAddressHost(),
          server.getAddressPort());
      transport.open();
      TProtocol protocol = new TBinaryProtocol(transport);
      LlamaAMService.Client client = new LlamaAMService.Client(protocol);
      ....
    } finally {
      server.stop();
    }   
  }
}
+----+
  
  Example of a minimal <<<fair-scheduler.xml>>> configuration file
  defining 3 queues:
  
+----+
<allocations>
  <queue name="queue1">
  </queue>
  <queue name="queue2">
  </queue>
  <queue name="queue3">
  </queue>
</allocations>
+----+

* Using MiniLlama from the Command-Line

  <<IMPORTANT:>> MiniLlama from the command-line is available from the
  mini-llama tarball, it is not available in the llama-dist tarball.

  MiniLlama requires a local Hadoop cluster installation to pick up the Hadoop
  JAR files. The <<<HADOOP_HOME>>> environment variable must be defined.

** Install MiniLlama

  Expand the <<<mini-llama-${project.version}.tar.gz>>>.

** Configure MiniLlama

  MiniLlama uses the following 2 configuration files from Llama installation
  directory:

  * <<<conf/llama-site.xml>>>

  * <<<conf/llama-log4j.properties>>>

** Run MiniLlama

  To run MiniLlama use the <<<minillama>>> script:

+---+
$ bin/minillama --hadoop-conf=/Users/tucu/myhadoopconf --hadoop-nodes=3
+---+

  The 2 options shown are required.

  The <<<--hadoop-conf>>> option must point to a Hadoop configuration defining
  the configuration for the HDFS minicluster and Yarn minicluster.

  <<IMPORTANT:>> Hadoop core or hdfs configuration files must define the
  <<<fs.defaultFS>>> property.

  <<IMPORTANT:>> The yarn configuration must define the scheduler configuration
  and it should not include Map-Reduce's <<<ShuffleHandler>>> auxiliary service
  to avoid port conflicts.

  The <<<--hadoop-nodes>>> option indicates how many DataNodes and NodeManagers
  the minicluster should have.

  Once HDFS, Yarn and Llama are running the logs will output the following
  message:

+---+
2013-09-02 09:14:47,227 INFO  [main] server.MiniLlama (MiniLlama.java:main(62)) - *********************************************************************************************************************
2013-09-02 09:14:47,244 INFO  [main] server.MiniLlama (MiniLlama.java:main(64)) - Mini Llama running with HDFS/Yarn minicluster with 2 nodes, HDFS URI: hdfs://localhost:8020 Llama URI: 0.0.0.0:15000
2013-09-02 09:14:47,244 INFO  [main] server.MiniLlama (MiniLlama.java:main(68)) - *********************************************************************************************************************
+---+

  Additional options:
  
    * <<<-no-format>>>: It does not reformats MiniHDFS on startup.
    * <<<-write-hdfs-config=>>><<FILE>>: Writes out MiniHDFS configuration to 
    the specified <<FILE>>.
